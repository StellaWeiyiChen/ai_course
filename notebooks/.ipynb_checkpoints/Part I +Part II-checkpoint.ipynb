{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM AI Enterprise Workflow Capstone Project Part I\n",
    "\n",
    "### 1) Assimilate the business scenario and articulate testable hypotheses\n",
    "\n",
    "###### Business Scenario: \n",
    "\n",
    "AAVAIL's management wants to explore different ways of revenue generation than the initial subscription-based model. They carried out an experiement using \"Ã  la carte\" approach. During the experiement, they've been collecting transaction-level purchase data during a couple of years across 38 countries. \n",
    "\n",
    "However, the reveue prediction of this new model is time consuming and the management team expects using data science to build accurate models to predict monthly revenue, as well as revenue from a specific country. \n",
    "\n",
    "The mangement team assured that well-projected numbers will help stabilize staffing and budget projections which will have a beneficial ripple effect throughout the company.\n",
    "\n",
    "###### Business metrics:\n",
    "\n",
    "- % of improvement of accuracy in budget projections in comparison to those predicted by the management team\n",
    "\n",
    "- Amount of time saved during revenue prediction using ML models\n",
    "\n",
    "###### My Null Hypothesis: \n",
    "- The prediction using ML models can not be more accurate than the predictions performed by the management team\n",
    "\n",
    "- The implementation of ML models doesn't save time during revenue prediction\n",
    "\n",
    "### 2)  State the ideal data to address the business opportunity and clarify the rationale for needing specific data.\n",
    "\n",
    "The ideal data would be a set of clean data which contains all those features that the management team considered when projecting revenue using their method, within which should be country as feature and revenue as target variable. The set is based on the features selected by business experts and can be directly used to build a supervised learning model. \n",
    "\n",
    "\n",
    "### 3) Create a python script to extract relevant data from multiple data sources, automating the process of data ingestion.\n",
    "\n",
    "See data_ingestion.py or get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, io\n",
    "import glob\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./../data/cs-train/\"\n",
    "all_files=glob.glob(os.path.join(path,\"*.json\"))\n",
    "df=pd.concat((pd.read_json(f) for f in all_files))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first challenge is to concatenate all the jsons in a single dataframe. However, we got warning about the non-alignment of concatenation axis, which means inconsistents column names within different json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dictionary={\n",
    "    \"StreamID\":\"stream_id\",\n",
    "    \"TimesViewed\":\"times_viewed\",\n",
    "    \"total_price\":\"price\"\n",
    "}\n",
    "df=pd.concat((pd.read_json(f).rename(columns=rename_dictionary) for f in all_files))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the column year_month_day in an only column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"]=df.year.astype(str)+\"-\"+df.month.astype(str).str.zfill(2)+\"-\"+df.day.astype(str).str.zfill(2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    all_files=glob.glob(os.path.join(path,\"*.json\"))\n",
    "    rename_dictionary={\n",
    "        \"StreamID\":\"stream_id\",\n",
    "        \"TimesViewed\":\"times_viewed\",\n",
    "        \"total_price\":\"price\"}\n",
    "    df=pd.concat((pd.read_json(f).rename(columns=rename_dictionary) for f in all_files))\n",
    "    df[\"date\"]=df.year.astype(str)+\"-\"+df.month.astype(str).str.zfill(2)+\"-\"+df.day.astype(str).str.zfill(2)\n",
    "    df.invoice=df.invoice.astype(str).str.replace(\"[a-zA-Z]\",\"\")\n",
    "    return df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Investigate the relationship between the relevant data, the target and the business metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Know about duplications and missing data\n",
    "- Drop duplicate data\n",
    "- Explore missing values\n",
    "\n",
    "#### 4.2 Relationship between different columns\n",
    "- Relationship between customer_id, stream_id, times_viewed and price\n",
    "- Explore the behaviour of different countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before drop duplicates\")\n",
    "print(collections.Counter(df.duplicated()))\n",
    "df=df.drop_duplicates()\n",
    "print(\"After dropped duplicates\")\n",
    "print(collections.Counter(df.duplicated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Drop invoice ids with letters\")\n",
    "df.invoice=df.invoice.astype(str).str.replace(\"[a-zA-Z]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Explore missing values\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.customer_id.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must decide what to do with the null values,  firstly the amount of missing values in customer_id is meaningful, and since our target variable is price aggreagate by country and by month, we could keep these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby([\"customer_id\"]).agg({\"invoice\": pd.Series.nunique}).head())\n",
    "df.groupby([\"invoice\"]).agg({\"stream_id\":pd.Series.nunique}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single customer have different invoices and a invoice have different stream_ids which reference to the contents the customer have seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g=pd.DataFrame(df.groupby([\"country\"]).price.sum()).reset_index().sort_values(by=\"price\",ascending=False)\n",
    "sns.catplot(data=df_g[:15], y=\"country\",x=\"price\",kind=\"bar\")\n",
    "sorted_countries=df_g.country.values\n",
    "plt.title(\"Top 15 countries with highest total price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe high differences between the total price which could be influenced by the number of customers, so we normalize the prices to price per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g=pd.DataFrame(df.groupby([\"country\"]).agg({\n",
    "    \"price\":\"sum\",\n",
    "    \"customer_id\":pd.Series.nunique\n",
    "})).reset_index()\n",
    "df_g[\"prices_customer\"]=np.divide(df_g.price,df_g.customer_id,out=np.zeros_like(df_g.price), where=df_g.customer_id!=0)\n",
    "df_g=df_g.sort_values(by=\"prices_customer\",ascending=False).rename(columns={\n",
    "    \"customer_id\":\"n_customers\"\n",
    "})\n",
    "sorted_countries_mean=df_g.country.values\n",
    "sns.catplot(data=df_g.sort_values(by=\"prices_customer\",ascending=False)[:15], y=\"country\",x=\"prices_customer\",kind=\"bar\")\n",
    "plt.title(\"Top 15 countries with highest prices per customer\")\n",
    "print(df_g[:15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were two countries with non customers which was no considered for price per customer calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_uk=df_g[df_g.country==\"United Kingdom\"].price/df_g.price.sum()\n",
    "\n",
    "p_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g[df_g.n_customers==0].country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "g=sns.relplot(\n",
    "    data=df_g, \n",
    "    x=\"n_customers\",y=\"prices_customer\")\n",
    "plt.xlim((0,40))\n",
    "plt.ylim(0,700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g.loc[df_g.prices_customer>800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EIRE, Singapore and Norway has high prices per customer but with very few customers. If we remember that there were missing values in customer_ids, we can expect to find these customer_ids with these countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.customer_id.isna()].country.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also discovered here that there where data with country Unspecified, we must filter these data when predicting revenue for countries.\n",
    "\n",
    "An other way to scale the total price is with the number of invoices, in this case, we have the advantage of that we wouldn't need to deal with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g=pd.DataFrame(df.groupby([\"country\"]).agg({\n",
    "    \"price\":\"sum\",\n",
    "    \"invoice\":pd.Series.nunique\n",
    "})).reset_index()\n",
    "df_g[\"prices_invoice\"]=np.divide(df_g.price,df_g.invoice,out=np.zeros_like(df_g.price), where=df_g.invoice!=0)\n",
    "df_g=df_g.sort_values(by=\"prices_invoice\",ascending=False).rename(columns={\n",
    "    \"invoice\":\"n_invoice\"\n",
    "})\n",
    "sorted_countries_mean=df_g.country.values\n",
    "sns.catplot(data=df_g.sort_values(by=\"prices_invoice\",ascending=False)[:15], y=\"country\",x=\"prices_invoice\",kind=\"bar\")\n",
    "plt.title(\"Top 15 countries with highest prices per invoice\")\n",
    "print(df_g[:15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g=pd.DataFrame(df.groupby([\"country\",\"year\"]).agg({\n",
    "    \"price\":\"mean\"\n",
    "})).reset_index().sort_values(by=\"price\",ascending=False)\n",
    "plt.figure(figsize=(14,6))\n",
    "df_g=df_g[df_g.country.isin(sorted_countries[:15])]\n",
    "sns.catplot(data=df_g, x=\"price\",y=\"country\",hue=\"year\", kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g=pd.DataFrame(df.groupby([\"country\",\"year\"]).agg({\n",
    "    \"times_viewed\":\"mean\"\n",
    "})).reset_index().sort_values(by=\"times_viewed\",ascending=False)\n",
    "plt.figure(figsize=(14,6))\n",
    "df_g=df_g[df_g.country.isin(sorted_countries[:15])]\n",
    "sns.catplot(data=df_g, x=\"times_viewed\",y=\"country\",hue=\"year\", kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date=pd.to_datetime(df.date)\n",
    "df_g=df.groupby([\"year\",\"month\",\"country\"]).price.sum().reset_index()\n",
    "df_g[\"year_month\"]=df_g.year.astype(str)+\"-\"+df_g.month.astype(str).str.zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.lineplot(x=\"year_month\",y=\"price\",data=df_g[df_g.country.isin(sorted_countries[:10])],hue=\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g=df.groupby([\"date\",\"country\"]).price.sum().reset_index()\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.lineplot(x=\"date\",y=\"price\",data=df_g[df_g.country.isin(sorted_countries[:10])],hue=\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "df_g=df.groupby([\"date\"]).price.sum().reset_index()\n",
    "X = df_g.price.values\n",
    "result = adfuller(X)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "\tprint('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Articulate your findings using a deliverable with visualizations.\n",
    "- We discovered that the United Kindong is obviously the main market of the company,around 90% of the whole revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_countries[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These are the top 15 countries with highest revenue ordered from higher to lower\n",
    "- We find the highest revenue per costumber in EIRE, Singapore and Norway, and the highest revenue per invoice in HongKong, Singapore and Norway. \n",
    "- We also understood that the majority of the countries has their price per customer under 1000, except EIRE, Singapore, Norway, RSA and Malta. \n",
    "- The AD stationarity test turns that the daily acumulated price is stationary.\n",
    "- The daily price plot per country suggested that the data from UK is stationary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Time Series Analysis\n",
    "\n",
    "##### 1) State the different modeling approaches that you will compare to address the business opportunity.\n",
    "I will compapre autoregressive models  and Facebook prophet\n",
    "##### 2) Iterate on your suite of possible models by modifying data transformations, pipeline architectures, hyperparameters and other relevant factors.\n",
    "##### 3) Re-train your model on all of the data using the selected approach and prepare it for deployment.\n",
    "##### 4) Articulate your findings in a summary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge fbprophet -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2aa68ed5927c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautocorrelation_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfbprophet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProphet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from data_ingestion import *\n",
    "from statsmodels.tsa.ar_model import AutoReg, ar_select_order, AR\n",
    "from statsmodels.tsa.api import acf, pacf, graphics\n",
    "from statsmodels.graphics.tsaplots import plot_predict\n",
    "from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from pandas.plotting import lag_plot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password:"
     ]
    }
   ],
   "source": [
    "!sudo pip install fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./../data/cs-train/\"\n",
    "df=get_data(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily=df.groupby([\"date\"]).price.sum().reset_index()\n",
    "df_daily[\"date\"]=df_daily.date.astype(np.datetime64)\n",
    "df_daily.index=df_daily.date\n",
    "print(df_daily.shape)\n",
    "df_daily=df_daily.asfreq(freq=\"1D\",fill_value=1)\n",
    "print(df_daily.shape)\n",
    "df_daily=df_daily[[\"price\"]]\n",
    "df_daily.price.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(df_daily)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.concat([df.price.shift(1),df.price],axis=1)\n",
    "result.columns=[\"t-1\",\"t+1\"]\n",
    "result=result.corr()\n",
    "print(\"Correlation for lag =1\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "autocorrelation_plot(df_daily)\n",
    "plt.title(\"Correlation lag plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(df_daily.price,lags=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model AUTOREG  LAGS=16\n",
    "X=df_daily.values\n",
    "train, test = X[1:len(X)-7], X[len(X)-7:]\n",
    "model=AutoReg(train, lags=16)\n",
    "res=model.fit()\n",
    "predictions = res.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "for i in range(len(predictions)):\n",
    "    print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n",
    "rmse = np.sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "plt.plot(test)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model AR\n",
    "X=df_daily.values\n",
    "#train, test = df_daily[1:len(X)-7], X[len(X)-7:]\n",
    "model=ARIMA(df_daily,order=(2,0,2),trend=\"n\")\n",
    "res=model.fit()\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "fig = plot_predict(res, start='2018-01-01', end='2019-07-30', ax=ax)\n",
    "legend = ax.legend(loc='upper left')\n",
    "predictions = res.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "rmse = np.sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,9))\n",
    "fig = res.plot_diagnostics(fig=fig,lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ar_model=AutoReg(df_daily, lags=16)\n",
    "res_ar=ar_model.fit()\n",
    "arima_model=ARIMA(df_daily,order=(2,0,2),trend=\"n\")\n",
    "res_arima=arima_model.fit()\n",
    "predictions = pd.DataFrame({\"AR(16)\": res_ar.predict(start='2018-01-01', end='2019-07-30'),\n",
    "                            \"ARIMA\": res_arima.predict(start='2018-01-01', end='2019-07-30')}\n",
    "                          )\n",
    "ax=df_daily.price.plot()\n",
    "_, ax = plt.subplots()\n",
    "ax = predictions.plot(ax=ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_predict(data, period, changepoint_scale=0.5):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['ds'] = data.index\n",
    "    df['y'] = data.price.values\n",
    "    m = Prophet(changepoint_prior_scale=changepoint_scale)\n",
    "    m_fit = m.fit(df)\n",
    "    future = m.make_future_dataframe(periods = period)\n",
    "    forecast = m.predict(future)\n",
    "    forecast = forecast.round(0)\n",
    "    fig1 = m.plot(forecast)\n",
    "    fig2 = m.plot_components(forecast)\n",
    "\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prophet_predict(df_daily,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
